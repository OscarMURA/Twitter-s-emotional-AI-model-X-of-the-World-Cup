{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIxnzleDxxGL"
      },
      "source": [
        "# FIFA World Cup 2022 Tweets - Limpieza y Preprocesamiento con NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE94IjMIWw11",
        "outputId": "8c68ad7b-792c-4ec3-9e8d-292af94fa548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: numpy in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (2.2.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: emoji in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (2.14.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "%pip install numpy\n",
        "%pip install emoji\n",
        "%pip install pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd # Importar pandas y asignarle el alias pd\n",
        "import re\n",
        "import emoji\n",
        "import sys          # Importar sys\n",
        "import os\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7W6W-iA5AKU"
      },
      "source": [
        "#  Descargar recursos de NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8FQs54_3F42",
        "outputId": "c6143693-325a-4148-c657-513950d4531c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verificando/Descargando recursos de NLTK ('punkt', 'stopwords')...\n"
          ]
        }
      ],
      "source": [
        "# --- Descargar recursos de NLTK (solo necesitas hacer esto una vez) ---\n",
        "# Estos son necesarios para la tokenización y remoción de stopwords\n",
        "print(\"Verificando/Descargando recursos de NLTK ('punkt', 'stopwords')...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except (nltk.downloader.DownloadError, LookupError):\n",
        "    print(\"Recurso 'punkt' de NLTK no encontrado. Intentando descargar...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except (nltk.downloader.DownloadError, LookupError):\n",
        "    print(\"Recurso 'stopwords' de NLTK no encontrado. Intentando descargar...\")\n",
        "    nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98WqedOi5Hjj"
      },
      "source": [
        "# Cargar las stopwords una vez\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2zRfZNb3PR2",
        "outputId": "15c116a3-eb69-41c2-c228-5eb8b6f484b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recursos de NLTK 'punkt' y 'stopwords' verificados/descargados.\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "print(\"Recursos de NLTK 'punkt' y 'stopwords' verificados/descargados.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BmaAuOa5K5c"
      },
      "source": [
        "#  Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aZZAAfGr39KY"
      },
      "outputs": [],
      "source": [
        "# --- Configuración ---\n",
        "ruta_base = os.path.dirname(os.getcwd())\n",
        "input_folder = 'data'\n",
        "input_filename = 'fifa_world_cup_2022_tweets.csv'\n",
        "input_csv_file = os.path.join(ruta_base,input_folder, input_filename)\n",
        "\n",
        "\n",
        "output_folder = os.path.join(ruta_base,'data_processed') \n",
        "output_filename = 'fifa_tweets_clean.csv'\n",
        "output_csv_file = os.path.join(output_folder, output_filename)\n",
        "\n",
        "original_columns_expected = [\n",
        "    'Date Created',\n",
        "    'Number of Likes',\n",
        "    'Source of Tweet',\n",
        "    'Tweet',\n",
        "    'Sentiment'\n",
        "]\n",
        "\n",
        "sentiment_mapping = {\n",
        "    'negative': 0,\n",
        "    'neutral': 1,\n",
        "    'positive': 2\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ehHoXDc5NYk"
      },
      "source": [
        "#  Funciones de procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "q0Famip94CKT"
      },
      "outputs": [],
      "source": [
        "def extract_hashtags(tweet):\n",
        "    \"\"\"Extrae hashtags de un tweet.\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return []\n",
        "    tweet_str = str(tweet)\n",
        "    hashtags = re.findall(r'#(\\w+)', tweet_str)\n",
        "    return hashtags\n",
        "\n",
        "def get_emoji_descriptions(tweet):\n",
        "    \"\"\"Convierte emojis en texto descriptivo (ej: ❤️ -> red heart).\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return []\n",
        "    tweet_str = str(tweet)\n",
        "    emoji_list_found = emoji.emoji_list(tweet_str)\n",
        "    descriptions = []\n",
        "    for emo in emoji_list_found:\n",
        "        description = emoji.demojize(emo['emoji'], delimiters=(\"\", \"\")).lower()\n",
        "        descriptions.append(description)\n",
        "    return descriptions\n",
        "\n",
        "def extract_emoji_chars(tweet):\n",
        "    \"\"\"Extrae solo los caracteres de los emojis de un tweet.\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return []\n",
        "    tweet_str = str(tweet)\n",
        "    emoji_list_found = emoji.emoji_list(tweet_str)\n",
        "    chars = [emo['emoji'] for emo in emoji_list_found]\n",
        "    return chars\n",
        "\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    \"\"\"\n",
        "    Limpia un tweet: elimina URLs, convierte emojis y hashtags a tokens especiales,\n",
        "    mantiene menciones sin arroba, y elimina caracteres especiales generales\n",
        "    permitiendo guiones bajos y números para los tokens especiales.\n",
        "    \"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(tweet)\n",
        "\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "    text = re.sub(r'pic.twitter.com/\\S+', '', text)\n",
        "    text = re.sub(r't.co/\\S+', '', text)\n",
        "\n",
        " \n",
        "    text = re.sub(r'\\B@(\\w+)', r'\\1', text)\n",
        "\n",
        "    emoji_list = emoji.emoji_list(text)\n",
        "    for emo in reversed(emoji_list):\n",
        "        start, end = emo['match_start'], emo['match_end']\n",
        "        emo_char = emo['emoji']\n",
        "        desc = emoji.demojize(emo_char, delimiters=(\"\", \"\")).replace(\" \", \"_\")\n",
        "        special_token = f\" _EMOJI_{desc}_ \"\n",
        "        text = text[:start] + special_token + text[end:]\n",
        "\n",
        "\n",
        "    text = re.sub(r'\\B#(\\w+)', r' _HASHTAG_\\1_ ', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s_]+', '', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def process_text_for_ml(text):\n",
        "    \"\"\"\n",
        "    Procesa texto limpio: aplica tokenización, remoción de stopwords,\n",
        "    y filtra palabras cortas, conservando tokens especiales de emoji y hashtag.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
        "        return []\n",
        "    tokens = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "    stop_words_lower = set(word.lower() for word in stop_words)\n",
        "\n",
        "    emoji_prefix = \"_emoji_\"\n",
        "    hashtag_prefix = \"_hashtag_\"\n",
        "\n",
        "    processed_tokens = [\n",
        "        word for word in tokens\n",
        "        if word not in stop_words_lower and (len(word) > 1 or word.startswith(emoji_prefix) or word.startswith(hashtag_prefix))\n",
        "    ]\n",
        "\n",
        "    return processed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDrBq-jx5TBt"
      },
      "source": [
        "#  Procesamiento principal: Carga y Preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r435RlL_4l85",
        "outputId": "f7c0d8ae-1608-4758-ca7a-30e7c5b28abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando dataset desde 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data\\fifa_world_cup_2022_tweets.csv'...\n",
            "Dataset cargado exitosamente. Número de filas: 22524\n",
            "Iniciando procesamiento mejorado de tweets...\n",
            "Columna 'test_clean' (texto limpio con tokens especiales) creada.\n",
            "Columna 'processed_tokens' (lista de tokens procesados) creada.\n",
            "Columna 'sentiment_label' (numérica) creada.\n",
            "Procesamiento mejorado de tweets completado.\n",
            "\n",
            "--- Guardando dataset procesado para ML ---\n"
          ]
        }
      ],
      "source": [
        "print(f\"Cargando dataset desde '{input_csv_file}'...\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(\n",
        "        input_csv_file,\n",
        "        encoding='utf-8',\n",
        "        encoding_errors='replace',\n",
        "        engine='python',\n",
        "        on_bad_lines='skip'\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset cargado exitosamente. Número de filas: {len(df)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: El archivo '{input_csv_file}' no fue encontrado.\")\n",
        "    print(f\"Verifica la ruta: '{input_csv_file}'. Asegúrate de que el archivo CSV esté en la carpeta '{input_folder}' dentro del directorio donde ejecutas el script, o ajusta la ruta.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error inesperado al cargar el archivo CSV: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if not all(col in df.columns for col in original_columns_expected):\n",
        "    missing = [col for col in original_columns_expected if col not in df.columns]\n",
        "    print(f\"Error: Faltan columnas requeridas en el dataset de entrada: {missing}\")\n",
        "    print(f\"Columnas disponibles: {df.columns.tolist()}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "df['Tweet'] = df['Tweet'].fillna('').astype(str) # Asegurarse de que la columna Tweet es string\n",
        "df['Sentiment'] = df['Sentiment'].fillna('unknown').astype(str) # Asegurarse de que la columna Sentiment es string\n",
        "\n",
        "print(\"Iniciando procesamiento mejorado de tweets...\")\n",
        "\n",
        "# --- Aplicar las funciones de preprocesamiento ---\n",
        "\n",
        "df['test_clean'] = df['Tweet'].apply(clean_tweet)\n",
        "print(\"Columna 'test_clean' (texto limpio con tokens especiales) creada.\")\n",
        "\n",
        "df['processed_tokens'] = df['test_clean'].apply(process_text_for_ml)\n",
        "print(\"Columna 'processed_tokens' (lista de tokens procesados) creada.\")\n",
        "\n",
        "df['sentiment_label'] = df['Sentiment'].map(sentiment_mapping)\n",
        "\n",
        "df['sentiment_label'] = df['sentiment_label'].fillna(-1).astype(int)\n",
        "print(\"Columna 'sentiment_label' (numérica) creada.\")\n",
        "\n",
        "print(\"Procesamiento mejorado de tweets completado.\")\n",
        "\n",
        "print(f\"\\n--- Guardando dataset procesado para ML ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RUvUqDy5YJg"
      },
      "source": [
        "#  Seleccionar y guardar las columnas procesadas ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62cJ1Llzy23Y",
        "outputId": "4f52a78c-c657-4153-e039-4c49d7017baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Guardando dataset procesado para ML ---\n",
            "Carpeta de salida 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed' asegurada.\n",
            "Dataset guardado exitosamente en 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed\\fifa_tweets_clean.csv'.\n",
            "\n",
            "Script de preprocesamiento para ML completado.\n",
            "Columnas en el archivo de salida: ['Number of Likes', 'Tweet', 'Sentiment', 'test_clean', 'processed_tokens', 'sentiment_label']\n",
            "                                               Tweet  \\\n",
            "0  What are we drinking today @TucanTribe \\n@MadB...   \n",
            "1  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   \n",
            "2  Worth reading while watching #WorldCup2022 htt...   \n",
            "3  Golden Maknae shinning bright\\n\\nhttps://t.co/...   \n",
            "4  If the BBC cares so much about human rights, h...   \n",
            "\n",
            "                                          test_clean  \\\n",
            "0  what are we drinking today tucantribe madbears...   \n",
            "1  amazing canadasocceren _hashtag_worldcup2022_ ...   \n",
            "2  worth reading while watching _hashtag_worldcup...   \n",
            "3  golden maknae shinning bright _hashtag_jeonjun...   \n",
            "4  if the bbc cares so much about human rights ho...   \n",
            "\n",
            "                                    processed_tokens Sentiment  \\\n",
            "0  [drinking, today, tucantribe, madbears_, lkinc...   neutral   \n",
            "1  [amazing, canadasocceren, _hashtag_worldcup202...  positive   \n",
            "2  [worth, reading, watching, _hashtag_worldcup20...  positive   \n",
            "3  [golden, maknae, shinning, bright, _hashtag_je...  positive   \n",
            "4  [bbc, cares, much, human, rights, homosexual, ...  negative   \n",
            "\n",
            "   sentiment_label  \n",
            "0                1  \n",
            "1                2  \n",
            "2                2  \n",
            "3                2  \n",
            "4                0  \n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n--- Guardando dataset procesado para ML ---\")\n",
        "\n",
        "final_output_columns_order = [\n",
        "    'Number of Likes',\n",
        "    'Tweet',\n",
        "    'Sentiment',\n",
        "    'test_clean',\n",
        "    'processed_tokens', \n",
        "    'sentiment_label'    \n",
        "]\n",
        "\n",
        "\n",
        "final_output_columns_existing = [col for col in final_output_columns_order if col in df.columns]\n",
        "\n",
        "if len(final_output_columns_existing) < len(final_output_columns_order):\n",
        "    missing = [col for col in final_output_columns_order if col not in df.columns]\n",
        "    print(f\"Advertencia: Las siguientes columnas esperadas para guardar NO se encontraron en el dataset: {missing}\")\n",
        "    print(\"Guardando solo las columnas disponibles en la lista.\")\n",
        "   \n",
        "\n",
        "try:\n",
        "   \n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    print(f\"Carpeta de salida '{output_folder}' asegurada.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear la carpeta de salida '{output_folder}': {e}\")\n",
        "    sys.exit(1) \n",
        "\n",
        "try:\n",
        "    df[final_output_columns_existing].to_csv(output_csv_file, index=False, encoding='utf-8')\n",
        "    print(f\"Dataset guardado exitosamente en '{output_csv_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al guardar el archivo de salida '{output_csv_file}': {e}\")\n",
        "    print(\"Asegúrate de tener permisos de escritura en la carpeta de salida.\")\n",
        "    sys.exit(1) \n",
        "\n",
        "print(\"\\nScript de preprocesamiento para ML completado.\")\n",
        "print(f\"Columnas en el archivo de salida: {final_output_columns_existing}\")\n",
        "\n",
        "print(df[['Tweet', 'test_clean', 'processed_tokens', 'Sentiment', 'sentiment_label']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Guardando dataset procesado EDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Guardando dataset procesado para ML y EDA ---\n",
            "Carpeta de salida 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed' asegurada.\n",
            "Dataset guardado exitosamente en 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed\\fifa_tweets_clean_for_EDA.csv'.\n",
            "\n",
            "Script de preprocesamiento mejorado completado. Archivo listo para EDA y modelado.\n",
            "Columnas en el archivo de salida: ['Number of Likes', 'Tweet', 'Sentiment', 'test_clean', 'processed_tokens', 'hastag', 'emoji_chars', 'sentiment_label']\n",
            "\n",
            "Ejemplo de filas procesadas (incluyendo hastag y emoji_chars):\n",
            "                                               Tweet  \\\n",
            "0  What are we drinking today @TucanTribe \\n@MadB...   \n",
            "1  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   \n",
            "2  Worth reading while watching #WorldCup2022 htt...   \n",
            "3  Golden Maknae shinning bright\\n\\nhttps://t.co/...   \n",
            "4  If the BBC cares so much about human rights, h...   \n",
            "\n",
            "                                          test_clean  \\\n",
            "0  what are we drinking today tucantribe madbears...   \n",
            "1  amazing canadasocceren _hashtag_worldcup2022_ ...   \n",
            "2  worth reading while watching _hashtag_worldcup...   \n",
            "3  golden maknae shinning bright _hashtag_jeonjun...   \n",
            "4  if the bbc cares so much about human rights ho...   \n",
            "\n",
            "                                    processed_tokens  \\\n",
            "0  [drinking, today, tucantribe, madbears_, lkinc...   \n",
            "1  [amazing, canadasocceren, _hashtag_worldcup202...   \n",
            "2  [worth, reading, watching, _hashtag_worldcup20...   \n",
            "3  [golden, maknae, shinning, bright, _hashtag_je...   \n",
            "4  [bbc, cares, much, human, rights, homosexual, ...   \n",
            "\n",
            "                                              hastag emoji_chars Sentiment  \\\n",
            "0                                     [WorldCup2022]          []   neutral   \n",
            "1                                     [WorldCup2022]          []  positive   \n",
            "2                                     [WorldCup2022]          []  positive   \n",
            "3  [JeonJungkook, Jungkook, 전정국, 정국, JK, GoldenMa...          []  positive   \n",
            "4                                     [WorldCup2022]          []  negative   \n",
            "\n",
            "   sentiment_label  \n",
            "0                1  \n",
            "1                2  \n",
            "2                2  \n",
            "3                2  \n",
            "4                0  \n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n--- Guardando dataset procesado para ML y EDA ---\")\n",
        "\n",
        "\n",
        "output_folder = os.path.join(ruta_base,'data_processed') \n",
        "output_filename_eda = 'fifa_tweets_clean_for_EDA.csv'\n",
        "output_csv_file_eda = os.path.join(output_folder, output_filename_eda)\n",
        "\n",
        "final_output_columns_order_eda = [\n",
        "    'Number of Likes',\n",
        "    'Tweet',\n",
        "    'Sentiment', \n",
        "    'test_clean', \n",
        "    'processed_tokens', \n",
        "    'hastag',\n",
        "    'emoji_chars', \n",
        "    'sentiment_label'   \n",
        "]\n",
        "\n",
        "\n",
        "if 'hastag' not in df.columns:\n",
        "     df['hastag'] = df['Tweet'].astype(str).apply(extract_hashtags) \n",
        "     print(\"Columna 'hastag' generada para el output CSV.\")\n",
        "if 'emoji_chars' not in df.columns:\n",
        "     df['emoji_chars'] = df['Tweet'].astype(str).apply(extract_emoji_chars) \n",
        "     print(\"Columna 'emoji_chars' generada para el output CSV.\")\n",
        "\n",
        "final_output_columns_existing = [col for col in final_output_columns_order_eda if col in df.columns]\n",
        "\n",
        "if len(final_output_columns_existing) < len(final_output_columns_order_eda):\n",
        "    missing = [col for col in final_output_columns_order_eda if col not in df.columns]\n",
        "    print(f\"Advertencia: Las siguientes columnas esperadas para guardar NO se encontraron en el dataset: {missing}\")\n",
        "    print(\"Guardando solo las columnas disponibles en la lista.\")\n",
        "\n",
        "try:\n",
        "    \n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    print(f\"Carpeta de salida '{output_folder}' asegurada.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear la carpeta de salida '{output_folder}': {e}\")\n",
        "    sys.exit(1) \n",
        "\n",
        "try:\n",
        "    df[final_output_columns_existing].to_csv(output_csv_file_eda, index=False, encoding='utf-8')\n",
        "    print(f\"Dataset guardado exitosamente en '{output_csv_file_eda}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al guardar el archivo de salida '{output_csv_file_eda}': {e}\")\n",
        "    print(\"Asegúrate de tener permisos de escritura en la carpeta de salida.\")\n",
        "    sys.exit(1) \n",
        "\n",
        "print(\"\\nScript de preprocesamiento mejorado completado. Archivo listo para EDA y modelado.\")\n",
        "print(f\"Columnas en el archivo de salida: {final_output_columns_existing}\")\n",
        "\n",
        "\n",
        "print(\"\\nEjemplo de filas procesadas (incluyendo hastag y emoji_chars):\")\n",
        "print(df[['Tweet', 'test_clean', 'processed_tokens', 'hastag', 'emoji_chars', 'Sentiment', 'sentiment_label']].head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
