{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIxnzleDxxGL"
      },
      "source": [
        "# FIFA World Cup 2022 Tweets - Limpieza y Preprocesamiento con NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE94IjMIWw11",
        "outputId": "8c68ad7b-792c-4ec3-9e8d-292af94fa548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: emoji in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (2.14.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "%pip install emoji\n",
        "%pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pDi8544p3Dfq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import sys\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7W6W-iA5AKU"
      },
      "source": [
        "#  Descargar recursos de NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8FQs54_3F42",
        "outputId": "c6143693-325a-4148-c657-513950d4531c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verificando/Descargando recursos de NLTK ('punkt', 'stopwords')...\n"
          ]
        }
      ],
      "source": [
        "# --- Descargar recursos de NLTK ---\n",
        "print(\"Verificando/Descargando recursos de NLTK ('punkt', 'stopwords')...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except (nltk.downloader.DownloadError, LookupError):\n",
        "    print(\"Recurso 'punkt' de NLTK no encontrado. Intentando descargar...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except (nltk.downloader.DownloadError, LookupError):\n",
        "    print(\"Recurso 'stopwords' de NLTK no encontrado. Intentando descargar...\")\n",
        "    nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98WqedOi5Hjj"
      },
      "source": [
        "# Cargar las stopwords una vez\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2zRfZNb3PR2",
        "outputId": "15c116a3-eb69-41c2-c228-5eb8b6f484b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recursos de NLTK 'punkt' y 'stopwords' verificados/descargados.\n"
          ]
        }
      ],
      "source": [
        "# Cargar las stopwords una vez\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Recursos de NLTK 'punkt' y 'stopwords' verificados/descargados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BmaAuOa5K5c"
      },
      "source": [
        "#  Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aZZAAfGr39KY"
      },
      "outputs": [],
      "source": [
        "# --- Configuración ---\n",
        "ruta_base = os.path.dirname(os.getcwd())\n",
        "input_folder = 'data'\n",
        "input_filename = 'fifa_world_cup_2022_tweets.csv'\n",
        "input_csv_file = os.path.join(ruta_base,input_folder, input_filename)\n",
        "\n",
        "\n",
        "output_folder = os.path.join(ruta_base,'data_processed') \n",
        "output_filename = 'fifa_tweets_clean.csv'\n",
        "output_csv_file = os.path.join(output_folder, output_filename)\n",
        "\n",
        "original_columns_to_keep = [\n",
        "    'Date Created',\n",
        "    'Number of Likes',\n",
        "    'Source of Tweet',\n",
        "    'Tweet',\n",
        "    'Sentiment'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ehHoXDc5NYk"
      },
      "source": [
        "#  Funciones de procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "q0Famip94CKT"
      },
      "outputs": [],
      "source": [
        "def extract_hashtags(tweet):\n",
        "    \"\"\"Extrae hashtags de un tweet.\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return []\n",
        "    tweet_str = str(tweet)\n",
        "    hashtags = re.findall(r'#(\\w+)', tweet_str)\n",
        "    return hashtags\n",
        "\n",
        "def get_emoji_descriptions(tweet):\n",
        "    \"\"\"Convierte emojis en texto descriptivo (ej: ❤️ -> red heart).\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return []\n",
        "    tweet_str = str(tweet)\n",
        "    emoji_list_found = emoji.emoji_list(tweet_str)\n",
        "    descriptions = []\n",
        "    for emo in emoji_list_found:\n",
        "        description = emoji.demojize(emo['emoji'], delimiters=(\"\", \"\")).lower()\n",
        "        descriptions.append(description)\n",
        "    return descriptions\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    \"\"\"Limpia un tweet: elimina URLs, hashtags y emojis, y mantiene menciones sin arroba.\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(tweet)\n",
        "\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "    text = re.sub(r'pic.twitter.com/\\S+', '', text)\n",
        "    text = re.sub(r't.co/\\S+', '', text)\n",
        "\n",
        "    text = re.sub(r'\\B@(\\w+)', r'\\1', text)\n",
        "\n",
        "    text = re.sub(r'\\B#\\w+', '', text)\n",
        "\n",
        "    text = emoji.replace_emoji(text, '')\n",
        "\n",
        "    text = re.sub(r'[^A-Za-z\\s]+', '', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def process_text_for_ml(text):\n",
        "    \"\"\"Aplica tokenización, lowercasing y remoción de stopwords.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
        "        return []\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    #Remover caracteres no alfabéticos y palabras cortas después de tokenizar\n",
        "    # isalpha() verifica si contiene solo letras, len>1 quita palabras de 1 letra\n",
        "    tokens = [word for word in tokens if word.isalpha() and len(word) > 1]\n",
        "\n",
        "    #Remover Stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDrBq-jx5TBt"
      },
      "source": [
        "#  Procesamiento principal: Carga y Preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r435RlL_4l85",
        "outputId": "f7c0d8ae-1608-4758-ca7a-30e7c5b28abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando dataset desde 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data\\fifa_world_cup_2022_tweets.csv'...\n",
            "Dataset cargado exitosamente (usando encoding='utf-8' con manejo de errores, engine='python', saltando líneas con error).\n",
            "Nota: Caracteres no UTF-8 fueron reemplazados. Algunas líneas con formato incorrecto pueden haber sido omitidas durante la carga.\n",
            "Número de filas cargadas: 22524\n",
            "Iniciando procesamiento de tweets...\n",
            "Procesamiento de tweets completado.\n"
          ]
        }
      ],
      "source": [
        "# --- Procesamiento principal: Carga y Preprocesamiento ---\n",
        "print(f\"Cargando dataset desde '{input_csv_file}'...\")\n",
        "try:\n",
        "    df = pd.read_csv(\n",
        "        input_csv_file, # Usamos la ruta completa\n",
        "        encoding='utf-8',\n",
        "        encoding_errors='replace',\n",
        "        engine='python',\n",
        "        on_bad_lines='skip'\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset cargado exitosamente (usando encoding='utf-8' con manejo de errores, engine='python', saltando líneas con error).\")\n",
        "    print(f\"Nota: Caracteres no UTF-8 fueron reemplazados. Algunas líneas con formato incorrecto pueden haber sido omitidas durante la carga.\")\n",
        "    print(f\"Número de filas cargadas: {len(df)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: El archivo '{input_csv_file}' no fue encontrado.\")\n",
        "    print(f\"Asegúrate de que el archivo CSV esté en la carpeta '{input_folder}' dentro del directorio donde ejecutas el script, o proporciona la ruta completa correcta.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error inesperado al cargar el archivo CSV: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Asegurarse de que las columnas necesarias existen\n",
        "required_columns = original_columns_to_keep\n",
        "if not all(col in df.columns for col in required_columns):\n",
        "    missing = [col for col in required_columns if col not in df.columns]\n",
        "    print(f\"Error: Faltan columnas requeridas en el dataset: {missing}\")\n",
        "    print(f\"Columnas disponibles: {df.columns.tolist()}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "df['Tweet'] = df['Tweet'].fillna('')\n",
        "df['Sentiment'] = df['Sentiment'].fillna('unknown')\n",
        "\n",
        "print(\"Iniciando procesamiento de tweets...\")\n",
        "\n",
        "df['hastag'] = df['Tweet'].astype(str).apply(extract_hashtags)\n",
        "df['emojis'] = df['Tweet'].astype(str).apply(get_emoji_descriptions)\n",
        "\n",
        "df['test_clean'] = df['Tweet'].astype(str).apply(clean_tweet)\n",
        "\n",
        "\n",
        "df['processed_tokens'] = df['test_clean'].astype(str).apply(process_text_for_ml)\n",
        "\n",
        "print(\"Procesamiento de tweets completado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RUvUqDy5YJg"
      },
      "source": [
        "#  Seleccionar y guardar las columnas procesadas ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62cJ1Llzy23Y",
        "outputId": "4f52a78c-c657-4153-e039-4c49d7017baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Guardando dataset procesado ---\n",
            "Carpeta de salida 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed' asegurada.\n",
            "Dataset guardado exitosamente en 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed\\fifa_tweets_clean.csv'.\n",
            "\n",
            "Script de carga y preprocesamiento completado.\n"
          ]
        }
      ],
      "source": [
        "# --- Seleccionar y guardar las columnas procesadas ---\n",
        "print(f\"\\n--- Guardando dataset procesado ---\")\n",
        "\n",
        "cols_to_save = [col for col in (original_columns_to_keep + ['test_clean', 'hastag', 'emojis', 'processed_tokens']) if col in df.columns]\n",
        "\n",
        "try:\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    print(f\"Carpeta de salida '{output_folder}' asegurada.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear la carpeta de salida '{output_folder}': {e}\")\n",
        "\n",
        "try:\n",
        "    df[cols_to_save].to_csv(output_csv_file, index=False, encoding='utf-8')\n",
        "    print(f\"Dataset guardado exitosamente en '{output_csv_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al guardar el archivo de salida '{output_csv_file}': {e}\")\n",
        "    print(\"Asegúrate de tener permisos de escritura en la carpeta de salida.\")\n",
        "\n",
        "print(\"\\nScript de carga y preprocesamiento completado.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
