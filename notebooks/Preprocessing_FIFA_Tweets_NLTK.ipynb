{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIxnzleDxxGL"
      },
      "source": [
        "# FIFA World Cup 2022 Tweets - Limpieza y Preprocesamiento con NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE94IjMIWw11",
        "outputId": "8c68ad7b-792c-4ec3-9e8d-292af94fa548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 590.6/590.6 kB 3.0 MB/s eta 0:00:00\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: regex in c:\\users\\oscar\\documents\\semestre6\\discretas\\ti2-2025-1-lora_team\\.venv\\lib\\site-packages (2024.11.6)Note: you may need to restart the kernel to use updated packages.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#%pip install nltk\n",
        "#%pip install numpy\n",
        "%pip install emoji\n",
        "#%pip install pandas\n",
        "%pip install regex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd # Importar pandas y asignarle el alias pd\n",
        "import regex as re\n",
        "import emoji\n",
        "import sys          # Importar sys\n",
        "import os\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7W6W-iA5AKU"
      },
      "source": [
        "#  Descargar recursos de NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8FQs54_3F42",
        "outputId": "c6143693-325a-4148-c657-513950d4531c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verificando/Descargando recursos de NLTK ('punkt', 'stopwords')...\n"
          ]
        }
      ],
      "source": [
        "# --- Descargar recursos de NLTK (solo necesitas hacer esto una vez) ---\n",
        "# Estos son necesarios para la tokenización y remoción de stopwords\n",
        "print(\"Verificando/Descargando recursos de NLTK ('punkt', 'stopwords')...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except (nltk.downloader.DownloadError, LookupError):\n",
        "    print(\"Recurso 'punkt' de NLTK no encontrado. Intentando descargar...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except (nltk.downloader.DownloadError, LookupError):\n",
        "    print(\"Recurso 'stopwords' de NLTK no encontrado. Intentando descargar...\")\n",
        "    nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98WqedOi5Hjj"
      },
      "source": [
        "# Cargar las stopwords una vez\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2zRfZNb3PR2",
        "outputId": "15c116a3-eb69-41c2-c228-5eb8b6f484b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recursos de NLTK 'punkt' y 'stopwords' verificados/descargados.\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "print(\"Recursos de NLTK 'punkt' y 'stopwords' verificados/descargados.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BmaAuOa5K5c"
      },
      "source": [
        "#  Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aZZAAfGr39KY"
      },
      "outputs": [],
      "source": [
        "# --- Configuración ---\n",
        "ruta_base = os.path.dirname(os.getcwd())\n",
        "input_folder = 'data'\n",
        "input_filename = 'fifa_world_cup_2022_tweets.csv'\n",
        "input_csv_file = os.path.join(ruta_base,input_folder, input_filename)\n",
        "\n",
        "\n",
        "output_folder = os.path.join(ruta_base,'data_processed') \n",
        "output_filename = 'fifa_tweets_clean.csv'\n",
        "output_csv_file = os.path.join(output_folder, output_filename)\n",
        "\n",
        "original_columns_expected = [\n",
        "    'Date Created',\n",
        "    'Number of Likes',\n",
        "    'Source of Tweet',\n",
        "    'Tweet',\n",
        "    'Sentiment'\n",
        "]\n",
        "\n",
        "sentiment_mapping = {\n",
        "    'negative': 0,\n",
        "    'neutral': 1,\n",
        "    'positive': 2\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ehHoXDc5NYk"
      },
      "source": [
        "#  Funciones de procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q0Famip94CKT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: custom_stopwords_lower creado. Contiene 'not'? False\n",
            "DEBUG: custom_stopwords_lower creado. Contiene 'n't'? False\n",
            "DEBUG: custom_stopwords_lower creado. Contiene 'no'? False\n"
          ]
        }
      ],
      "source": [
        "def extract_hashtags(tweet):\n",
        "    \"\"\"Extrae hashtags de un tweet.\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return []\n",
        "    tweet_str = str(tweet)\n",
        "    hashtags = re.findall(r'#(\\w+)', tweet_str)\n",
        "    return hashtags\n",
        "\n",
        "def get_emoji_descriptions(tweet):\n",
        "    \"\"\"Convierte emojis en texto descriptivo (ej: ❤️ -> red heart).\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return []\n",
        "    tweet_str = str(tweet)\n",
        "    emoji_list_found = emoji.emoji_list(tweet_str)\n",
        "    descriptions = []\n",
        "    for emo in emoji_list_found:\n",
        "        description = emoji.demojize(emo['emoji'], delimiters=(\"\", \"\")).lower()\n",
        "        descriptions.append(description)\n",
        "    return descriptions\n",
        "\n",
        "def extract_emoji_chars(tweet):\n",
        "    \"\"\"Extrae solo los caracteres de los emojis de un tweet.\"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return []\n",
        "    tweet_str = str(tweet)\n",
        "    emoji_list_found = emoji.emoji_list(tweet_str)\n",
        "    chars = [emo['emoji'] for emo in emoji_list_found]\n",
        "    return chars\n",
        "\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    \"\"\"\n",
        "    Limpia un tweet: elimina URLs, convierte menciones, emojis y hashtags a tokens especiales,\n",
        "    y elimina caracteres generales permitiendo letras de cualquier idioma, números, espacios y guiones bajos.\n",
        "    \"\"\"\n",
        "    if pd.isna(tweet):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(tweet)\n",
        "\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "    text = re.sub(r'pic.twitter.com/\\S+', '', text)\n",
        "    text = re.sub(r't.co/\\S+', '', text)\n",
        "\n",
        "    text = re.sub(r'\\B@(\\w+)', r' _MENTION_\\1_ ', text)\n",
        "\n",
        "\n",
        "    emoji_list = emoji.emoji_list(text)\n",
        "    for emo in reversed(emoji_list):\n",
        "        start, end = emo['match_start'], emo['match_end']\n",
        "        emo_char = emo['emoji']\n",
        "        # Obtener descripción demojizada, reemplazar espacios por guiones bajos\n",
        "        # Lowercase la descripción\n",
        "        desc = emoji.demojize(emo_char, delimiters=(\"\", \"\")).replace(\" \", \"_\").lower()\n",
        "        # Crear token especial con espacios alrededor para ayudar a la tokenización\n",
        "        special_token = f\" _EMOJI_{desc}_ \"\n",
        "        text = text[:start] + special_token + text[end:]\n",
        "\n",
        "\n",
        "    text = re.sub(r'\\B#(\\w+)', lambda m: f\" _HASHTAG_{m.group(1).lower()}_ \", text)\n",
        "\n",
        "\n",
        "    text = re.sub(r'[^\\p{L}0-9\\s_]+', '', text, flags=re.UNICODE)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "try:\n",
        "    standard_stopwords = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"Error: NLTK stopwords no encontradas. Descargando...\")\n",
        "    nltk.download('stopwords')\n",
        "    standard_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "negation_words_lower = {\n",
        "    \"no\", \"not\", \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\",\n",
        "    \"hardly\", \"scarcely\", \"barely\", \"seldom\", \"rarely\",\n",
        "    \"don't\", \"doesn't\", \"didn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\",\n",
        "    \"haven't\", \"hasn't\", \"hadn't\", \"won't\", \"wouldn't\", \"can't\", \"cannot\",\n",
        "    \"couldn't\", \"shouldn't\", \"mightn't\", \"mustn't\",\n",
        "    \"n't\"\n",
        "}\n",
        "\n",
        "\n",
        "standard_stopwords_lower = set(word.lower() for word in standard_stopwords)\n",
        "\n",
        "custom_stopwords_lower = standard_stopwords_lower - negation_words_lower\n",
        "\n",
        "print(\"DEBUG: custom_stopwords_lower creado. Contiene 'not'?\", 'not' in custom_stopwords_lower)\n",
        "print(\"DEBUG: custom_stopwords_lower creado. Contiene 'n\\'t'?\", 'n\\'t' in custom_stopwords_lower)\n",
        "print(\"DEBUG: custom_stopwords_lower creado. Contiene 'no'?\", 'no' in custom_stopwords_lower)\n",
        "\n",
        "\n",
        "# --- Definir la función de procesamiento de texto con DEBUGGING ---\n",
        "def process_text_for_ml(text):\n",
        "    \"\"\"\n",
        "    Procesa texto limpio: aplica tokenización, remoción de stopwords (excluyendo negaciones),\n",
        "    y filtra palabras cortas, conservando tokens especiales de mencion, emoji y hashtag.\n",
        "\n",
        "    Argumentos:\n",
        "        text (str): La cadena de texto limpia (proveniente de 'test_clean').\n",
        "\n",
        "    Retorna:\n",
        "        list: Una lista de tokens procesados. Retorna lista vacía para inputs no válidos.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
        "        # print(\"DEBUG: Input text is invalid, returning [].\")\n",
        "        return []\n",
        "\n",
        "\n",
        "    tokens = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "    mention_prefix = \"_mention_\"\n",
        "    emoji_prefix = \"_emoji_\"\n",
        "    hashtag_prefix = \"_hashtag_\"\n",
        "\n",
        "    processed_tokens = []\n",
        "    for word in tokens:\n",
        "        word_lower = word.lower() # Convertir a minúsculas para comparación\n",
        "\n",
        "\n",
        "\n",
        "        # Condición 1: Verificar si es stopword (excluyendo negaciones)\n",
        "        is_stop = word_lower in custom_stopwords_lower\n",
        "\n",
        "\n",
        "\n",
        "        # Condición 2 & 3: Verificar longitud o si es token especial\n",
        "        is_special_token = word.startswith(mention_prefix) or word.startswith(emoji_prefix) or word.startswith(hashtag_prefix)\n",
        "        passes_length_check = len(word) > 1\n",
        "\n",
        "\n",
        "        # Lógica de filtrado combinada\n",
        "        if not is_stop and (passes_length_check or is_special_token):\n",
        "            processed_tokens.append(word)\n",
        "\n",
        "    return processed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDrBq-jx5TBt"
      },
      "source": [
        "#  Procesamiento principal: Carga y Preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r435RlL_4l85",
        "outputId": "f7c0d8ae-1608-4758-ca7a-30e7c5b28abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando dataset desde 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data\\fifa_world_cup_2022_tweets.csv'...\n",
            "Dataset cargado exitosamente. Número de filas: 22524\n",
            "Iniciando procesamiento mejorado de tweets...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columna 'test_clean' (texto limpio con tokens especiales) creada.\n",
            "Columna 'processed_tokens' (lista de tokens procesados) creada.\n",
            "Columna 'sentiment_label' (numérica) creada.\n",
            "Procesamiento mejorado de tweets completado.\n",
            "\n",
            "--- Guardando dataset procesado para ML ---\n"
          ]
        }
      ],
      "source": [
        "print(f\"Cargando dataset desde '{input_csv_file}'...\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(\n",
        "        input_csv_file,\n",
        "        encoding='utf-8',\n",
        "        encoding_errors='replace',\n",
        "        engine='python',\n",
        "        on_bad_lines='skip'\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset cargado exitosamente. Número de filas: {len(df)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: El archivo '{input_csv_file}' no fue encontrado.\")\n",
        "    print(f\"Verifica la ruta: '{input_csv_file}'. Asegúrate de que el archivo CSV esté en la carpeta '{input_folder}' dentro del directorio donde ejecutas el script, o ajusta la ruta.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error inesperado al cargar el archivo CSV: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if not all(col in df.columns for col in original_columns_expected):\n",
        "    missing = [col for col in original_columns_expected if col not in df.columns]\n",
        "    print(f\"Error: Faltan columnas requeridas en el dataset de entrada: {missing}\")\n",
        "    print(f\"Columnas disponibles: {df.columns.tolist()}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "df['Tweet'] = df['Tweet'].fillna('').astype(str) # Asegurarse de que la columna Tweet es string\n",
        "df['Sentiment'] = df['Sentiment'].fillna('unknown').astype(str) # Asegurarse de que la columna Sentiment es string\n",
        "\n",
        "print(\"Iniciando procesamiento mejorado de tweets...\")\n",
        "\n",
        "# --- Aplicar las funciones de preprocesamiento ---\n",
        "\n",
        "df['test_clean'] = df['Tweet'].apply(clean_tweet)\n",
        "print(\"Columna 'test_clean' (texto limpio con tokens especiales) creada.\")\n",
        "\n",
        "df['processed_tokens'] = df['test_clean'].apply(process_text_for_ml)\n",
        "print(\"Columna 'processed_tokens' (lista de tokens procesados) creada.\")\n",
        "\n",
        "df['sentiment_label'] = df['Sentiment'].map(sentiment_mapping)\n",
        "\n",
        "df['sentiment_label'] = df['sentiment_label'].fillna(-1).astype(int)\n",
        "print(\"Columna 'sentiment_label' (numérica) creada.\")\n",
        "\n",
        "print(\"Procesamiento mejorado de tweets completado.\")\n",
        "\n",
        "print(f\"\\n--- Guardando dataset procesado para ML ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RUvUqDy5YJg"
      },
      "source": [
        "#  Seleccionar y guardar las columnas procesadas ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62cJ1Llzy23Y",
        "outputId": "4f52a78c-c657-4153-e039-4c49d7017baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Guardando dataset procesado para ML ---\n",
            "Carpeta de salida 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed' asegurada.\n",
            "Dataset guardado exitosamente en 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed\\fifa_tweets_clean.csv'.\n",
            "\n",
            "Script de preprocesamiento para ML completado.\n",
            "Columnas en el archivo de salida: ['Tweet', 'Sentiment', 'test_clean', 'processed_tokens', 'sentiment_label']\n",
            "                                               Tweet  \\\n",
            "0  What are we drinking today @TucanTribe \\n@MadB...   \n",
            "1  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   \n",
            "2  Worth reading while watching #WorldCup2022 htt...   \n",
            "3  Golden Maknae shinning bright\\n\\nhttps://t.co/...   \n",
            "4  If the BBC cares so much about human rights, h...   \n",
            "\n",
            "                                          test_clean  \\\n",
            "0  what are we drinking today _mention_tucantribe...   \n",
            "1  amazing _mention_canadasocceren_ _hashtag_worl...   \n",
            "2  worth reading while watching _hashtag_worldcup...   \n",
            "3  golden maknae shinning bright _hashtag_jeonjun...   \n",
            "4  if the bbc cares so much about human rights ho...   \n",
            "\n",
            "                                    processed_tokens Sentiment  \\\n",
            "0  [drinking, today, _mention_tucantribe_, _menti...   neutral   \n",
            "1  [amazing, _mention_canadasocceren_, _hashtag_w...  positive   \n",
            "2  [worth, reading, watching, _hashtag_worldcup20...  positive   \n",
            "3  [golden, maknae, shinning, bright, _hashtag_je...  positive   \n",
            "4  [bbc, cares, much, human, rights, homosexual, ...  negative   \n",
            "\n",
            "   sentiment_label  \n",
            "0                1  \n",
            "1                2  \n",
            "2                2  \n",
            "3                2  \n",
            "4                0  \n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n--- Guardando dataset procesado para ML ---\")\n",
        "\n",
        "final_output_columns_order = [\n",
        "    'Tweet',\n",
        "    'Sentiment',\n",
        "    'test_clean',\n",
        "    'processed_tokens', \n",
        "    'sentiment_label'    \n",
        "]\n",
        "\n",
        "\n",
        "final_output_columns_existing = [col for col in final_output_columns_order if col in df.columns]\n",
        "\n",
        "if len(final_output_columns_existing) < len(final_output_columns_order):\n",
        "    missing = [col for col in final_output_columns_order if col not in df.columns]\n",
        "    print(f\"Advertencia: Las siguientes columnas esperadas para guardar NO se encontraron en el dataset: {missing}\")\n",
        "    print(\"Guardando solo las columnas disponibles en la lista.\")\n",
        "   \n",
        "\n",
        "try:\n",
        "   \n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    print(f\"Carpeta de salida '{output_folder}' asegurada.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear la carpeta de salida '{output_folder}': {e}\")\n",
        "    sys.exit(1) \n",
        "\n",
        "try:\n",
        "    df[final_output_columns_existing].to_csv(output_csv_file, index=False, encoding='utf-8')\n",
        "    print(f\"Dataset guardado exitosamente en '{output_csv_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al guardar el archivo de salida '{output_csv_file}': {e}\")\n",
        "    print(\"Asegúrate de tener permisos de escritura en la carpeta de salida.\")\n",
        "    sys.exit(1) \n",
        "\n",
        "print(\"\\nScript de preprocesamiento para ML completado.\")\n",
        "print(f\"Columnas en el archivo de salida: {final_output_columns_existing}\")\n",
        "\n",
        "print(df[['Tweet', 'test_clean', 'processed_tokens', 'Sentiment', 'sentiment_label']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Guardando dataset procesado EDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Guardando dataset procesado para ML y EDA ---\n",
            "Carpeta de salida 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed' asegurada.\n",
            "Dataset guardado exitosamente en 'c:\\Users\\oscar\\Documents\\Semestre6\\discretas\\ti2-2025-1-lora_team\\data_processed\\fifa_tweets_clean_for_EDA.csv'.\n",
            "\n",
            "Script de preprocesamiento mejorado completado. Archivo listo para EDA y modelado.\n",
            "Columnas en el archivo de salida: ['Number of Likes', 'Tweet', 'Sentiment', 'test_clean', 'processed_tokens', 'hastag', 'emoji_chars', 'sentiment_label']\n",
            "\n",
            "Ejemplo de filas procesadas (incluyendo hastag y emoji_chars):\n",
            "                                               Tweet  \\\n",
            "0  What are we drinking today @TucanTribe \\n@MadB...   \n",
            "1  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   \n",
            "2  Worth reading while watching #WorldCup2022 htt...   \n",
            "3  Golden Maknae shinning bright\\n\\nhttps://t.co/...   \n",
            "4  If the BBC cares so much about human rights, h...   \n",
            "\n",
            "                                          test_clean  \\\n",
            "0  what are we drinking today tucantribe madbears...   \n",
            "1  amazing canadasocceren _hashtag_worldcup2022_ ...   \n",
            "2  worth reading while watching _hashtag_worldcup...   \n",
            "3  golden maknae shinning bright _hashtag_jeonjun...   \n",
            "4  if the bbc cares so much about human rights ho...   \n",
            "\n",
            "                                    processed_tokens  \\\n",
            "0  [drinking, today, tucantribe, madbears_, lkinc...   \n",
            "1  [amazing, canadasocceren, _hashtag_worldcup202...   \n",
            "2  [worth, reading, watching, _hashtag_worldcup20...   \n",
            "3  [golden, maknae, shinning, bright, _hashtag_je...   \n",
            "4  [bbc, cares, much, human, rights, homosexual, ...   \n",
            "\n",
            "                                              hastag emoji_chars Sentiment  \\\n",
            "0                                     [WorldCup2022]          []   neutral   \n",
            "1                                     [WorldCup2022]          []  positive   \n",
            "2                                     [WorldCup2022]          []  positive   \n",
            "3  [JeonJungkook, Jungkook, 전정국, 정국, JK, GoldenMa...          []  positive   \n",
            "4                                     [WorldCup2022]          []  negative   \n",
            "\n",
            "   sentiment_label  \n",
            "0                1  \n",
            "1                2  \n",
            "2                2  \n",
            "3                2  \n",
            "4                0  \n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n--- Guardando dataset procesado para ML y EDA ---\")\n",
        "\n",
        "\n",
        "output_folder = os.path.join(ruta_base,'data_processed') \n",
        "output_filename_eda = 'fifa_tweets_clean_for_EDA.csv'\n",
        "output_csv_file_eda = os.path.join(output_folder, output_filename_eda)\n",
        "\n",
        "final_output_columns_order_eda = [\n",
        "    'Number of Likes',\n",
        "    'Tweet',\n",
        "    'Sentiment', \n",
        "    'test_clean', \n",
        "    'processed_tokens', \n",
        "    'hastag',\n",
        "    'emoji_chars', \n",
        "    'sentiment_label'   \n",
        "]\n",
        "\n",
        "\n",
        "if 'hastag' not in df.columns:\n",
        "     df['hastag'] = df['Tweet'].astype(str).apply(extract_hashtags) \n",
        "     print(\"Columna 'hastag' generada para el output CSV.\")\n",
        "if 'emoji_chars' not in df.columns:\n",
        "     df['emoji_chars'] = df['Tweet'].astype(str).apply(extract_emoji_chars) \n",
        "     print(\"Columna 'emoji_chars' generada para el output CSV.\")\n",
        "\n",
        "final_output_columns_existing = [col for col in final_output_columns_order_eda if col in df.columns]\n",
        "\n",
        "if len(final_output_columns_existing) < len(final_output_columns_order_eda):\n",
        "    missing = [col for col in final_output_columns_order_eda if col not in df.columns]\n",
        "    print(f\"Advertencia: Las siguientes columnas esperadas para guardar NO se encontraron en el dataset: {missing}\")\n",
        "    print(\"Guardando solo las columnas disponibles en la lista.\")\n",
        "\n",
        "try:\n",
        "    \n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    print(f\"Carpeta de salida '{output_folder}' asegurada.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear la carpeta de salida '{output_folder}': {e}\")\n",
        "    sys.exit(1) \n",
        "\n",
        "try:\n",
        "    df[final_output_columns_existing].to_csv(output_csv_file_eda, index=False, encoding='utf-8')\n",
        "    print(f\"Dataset guardado exitosamente en '{output_csv_file_eda}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al guardar el archivo de salida '{output_csv_file_eda}': {e}\")\n",
        "    print(\"Asegúrate de tener permisos de escritura en la carpeta de salida.\")\n",
        "    sys.exit(1) \n",
        "\n",
        "print(\"\\nScript de preprocesamiento mejorado completado. Archivo listo para EDA y modelado.\")\n",
        "print(f\"Columnas en el archivo de salida: {final_output_columns_existing}\")\n",
        "\n",
        "\n",
        "print(\"\\nEjemplo de filas procesadas (incluyendo hastag y emoji_chars):\")\n",
        "print(df[['Tweet', 'test_clean', 'processed_tokens', 'hastag', 'emoji_chars', 'Sentiment', 'sentiment_label']].head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
